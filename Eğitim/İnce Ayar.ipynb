{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# İnce Ayar\n",
    "Bu kod, ön eğitimden geçmiş veya geçmemiş, farklı transformer modellerinin farklı şekillerde BOUN Twitter Veri Seti üzerinden analizini içeriyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerekli kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, AdamW, BertForPreTraining, BertForMaskedLM, BertConfig, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from utils import url_removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerekli konfigürasyonlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = True   # Transformer modelini eğitirken embedding layerını dahil edip etmeme\n",
    "last_free_layer = 0      # Transformer modelinde eğitilecek son layer\n",
    "learning_rate = 1e-5     # Eğitim oranı\n",
    "weight_decay = 0.01      # Weight decay\n",
    "has_url_removal = True   # URL silmenin olduğu ön işleme\n",
    "optimizer_name = \"AdamW\" # Optimizer\n",
    "\n",
    "pretrained_path = \"None\"  # Ön eğitimde eğitilen modelin pathi, kullanılmayacaksa 'None'\n",
    "portion = 1 # Eğitim verisinde kullanılacak verinin oranı \n",
    "transformers_model = \"distilberturk\" # Kullanılacak transformer modeli (mbert, berturk, distilberturk)\n",
    "results_folder = f\"Results/{transformers_model}_{optimizer_name}/\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "results_filepath = os.path.join(results_folder, \"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Detayı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classifier', 'pooler', 'embedding', 'transformer.layer.0', 'transformer.layer.1', 'transformer.layer.2', 'transformer.layer.3', 'transformer.layer.4', 'transformer.layer.5', 'transformer.layer.6', 'transformer.layer.7', 'transformer.layer.8', 'transformer.layer.9', 'transformer.layer.10', 'transformer.layer.11']\n",
      "[FREE]: embeddings.word_embeddings.weight\n",
      "[FREE]: embeddings.position_embeddings.weight\n",
      "[FREE]: embeddings.LayerNorm.weight\n",
      "[FREE]: embeddings.LayerNorm.bias\n",
      "[FREE]: transformer.layer.0.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.0.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.0.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.0.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.0.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.0.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.0.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.0.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.0.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.0.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.0.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.0.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.0.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.0.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.0.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.0.output_layer_norm.bias\n",
      "[FREE]: transformer.layer.1.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.1.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.1.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.1.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.1.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.1.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.1.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.1.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.1.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.1.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.1.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.1.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.1.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.1.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.1.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.1.output_layer_norm.bias\n",
      "[FREE]: transformer.layer.2.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.2.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.2.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.2.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.2.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.2.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.2.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.2.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.2.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.2.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.2.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.2.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.2.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.2.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.2.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.2.output_layer_norm.bias\n",
      "[FREE]: transformer.layer.3.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.3.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.3.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.3.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.3.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.3.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.3.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.3.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.3.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.3.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.3.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.3.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.3.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.3.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.3.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.3.output_layer_norm.bias\n",
      "[FREE]: transformer.layer.4.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.4.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.4.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.4.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.4.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.4.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.4.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.4.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.4.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.4.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.4.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.4.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.4.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.4.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.4.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.4.output_layer_norm.bias\n",
      "[FREE]: transformer.layer.5.attention.q_lin.weight\n",
      "[FREE]: transformer.layer.5.attention.q_lin.bias\n",
      "[FREE]: transformer.layer.5.attention.k_lin.weight\n",
      "[FREE]: transformer.layer.5.attention.k_lin.bias\n",
      "[FREE]: transformer.layer.5.attention.v_lin.weight\n",
      "[FREE]: transformer.layer.5.attention.v_lin.bias\n",
      "[FREE]: transformer.layer.5.attention.out_lin.weight\n",
      "[FREE]: transformer.layer.5.attention.out_lin.bias\n",
      "[FREE]: transformer.layer.5.sa_layer_norm.weight\n",
      "[FREE]: transformer.layer.5.sa_layer_norm.bias\n",
      "[FREE]: transformer.layer.5.ffn.lin1.weight\n",
      "[FREE]: transformer.layer.5.ffn.lin1.bias\n",
      "[FREE]: transformer.layer.5.ffn.lin2.weight\n",
      "[FREE]: transformer.layer.5.ffn.lin2.bias\n",
      "[FREE]: transformer.layer.5.output_layer_norm.weight\n",
      "[FREE]: transformer.layer.5.output_layer_norm.bias\n",
      "Sentiment Net tanımlandı.\n"
     ]
    }
   ],
   "source": [
    "class Sentiment_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sentiment_Net, self).__init__()\n",
    "        if transformers_model==\"distilberturk\":\n",
    "            self.net_bert = AutoModel.from_pretrained('dbmdz/distilbert-base-turkish-cased')\n",
    "        elif transformers_model==\"berturk\":\n",
    "            self.net_bert = AutoModel.from_pretrained('dbmdz/bert-base-turkish-cased')\n",
    "        elif transformers_model==\"mbert\":\n",
    "            self.net_bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        unfrozen_layers = [\"classifier\", \"pooler\"]\n",
    "        if embedding_layer:\n",
    "            unfrozen_layers.append('embedding')\n",
    "        \n",
    "        for idx in range(last_free_layer, 12):\n",
    "            if transformers_model==\"distilberturk\":\n",
    "                unfrozen_layers.append('transformer.layer.'+str(idx))\n",
    "            elif transformers_model==\"mbert\" or transformers_model==\"berturk\":\n",
    "                unfrozen_layers.append('encoder.layer.'+str(idx))\n",
    "                \n",
    "            \n",
    "        print(unfrozen_layers)\n",
    "        for name, param in self.net_bert.named_parameters():\n",
    "            if not any([layer in name for layer in unfrozen_layers]):\n",
    "                print(\"[FROZE]: %s\" % name)\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                print(\"[FREE]: %s\" % name)\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, x, attention):\n",
    "        x, _ = self.net_bert(x, attention_mask=attention)\n",
    "\n",
    "        #Getting head\n",
    "        x = x[:,0,:]\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "try:\n",
    "    sentiment_net.apply(weight_reset)\n",
    "    print('Sentiment Net resetlendi.')\n",
    "except: \n",
    "    pass\n",
    "\n",
    "sentiment_net = Sentiment_Net().to(device)\n",
    "print('Sentiment Net tanımlandı.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformers_model==\"distilberturk\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('dbmdz/distilbert-base-turkish-cased')\n",
    "elif transformers_model==\"berturk\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-turkish-cased')\n",
    "elif transformers_model==\"mbert\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ön eğitim verisi yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None yok. Ön eğitim verisi yüklenmiyor.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(pretrained_path):\n",
    "    deneme = torch.load(pretrained_path,  map_location=torch.device('cpu'))\n",
    "    empty = {}\n",
    "    for k in deneme[\"model_state_dict\"].keys():\n",
    "        if k.startswith(\"fc1\"):\n",
    "            print(k)\n",
    "            continue\n",
    "        empty[k.replace(\"distilbert.\", \"\")] = deneme[\"model_state_dict\"][k]\n",
    "    sentiment_net.load_state_dict(empty, strict=False)\n",
    "    print(f\"Ön eğitim verisi {pretrained_path}'ten yüklendi.\")\n",
    "else:\n",
    "    print(f\"{pretrained_path} yok. Ön eğitim verisi yüklenmiyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Öznitelik çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_ext(json_fp, portion=1):\n",
    "    df = pd.read_json(json_fp).sample(frac=portion, random_state=10)\n",
    "    mapping = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "    y = [mapping[x] for x in df[\"value\"]]\n",
    "    \n",
    "    sentences = list(df[\"sentence\"])\n",
    "    if has_url_removal:\n",
    "        sentences = [url_removal(x) for x in sentences]\n",
    "    \n",
    "    \n",
    "    features = []\n",
    "    attention_masks = []\n",
    "    max_len = 256\n",
    "    for sentence in tqdm(sentences):\n",
    "        input_ids = torch.tensor(tokenizer.encode(sentence))\n",
    "        attention_mask = torch.cat((torch.tensor([1.0]*(len(input_ids))), torch.tensor([0.0]*(max_len-len(input_ids)))), 0)\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([0]*(max_len-len(input_ids)))), 0)        \n",
    "        attention_masks.append(attention_mask)\n",
    "        features.append(input_ids)\n",
    "    return torch.stack(features),torch.stack(attention_masks), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6326ea765bef4c37abb2710bafccd63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5733.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b74d41900b490a82ba8797e82018a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=639.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20707d1eab874d08bb727cd4c559a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1592.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_feat, X_train_attention, y_train = feat_ext(\"../Veriler/BOUN/train.json\", portion)\n",
    "X_dev_feat, X_dev_attention, y_dev = feat_ext(\"../Veriler/BOUN/validation.json\")\n",
    "X_test_feat, X_test_attention, y_test = feat_ext(\"../Veriler/BOUN/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer ve sınıf ağırlıklarının ayarlanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = compute_class_weight(class_weight=\"balanced\", classes=[0, 1, 2], y=y_train.tolist())\n",
    "criterion = nn.CrossEntropyLoss(torch.FloatTensor(cw).to(device))\n",
    "if optimizer_name == \"AdamW\":\n",
    "    optimizer = AdamW(sentiment_net.parameters(), lr=learning_rate,  correct_bias=False, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = optim.Adam(sentiment_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"SGD\":\n",
    "    optimizer = optim.SGD(sentiment_net.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eğitim Döngüsü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1_1,     8/716] loss: 1.121 accuracy: 0.344\n",
      "[1_2,    16/716] loss: 1.123 accuracy: 0.383\n",
      "[1_3,    24/716] loss: 1.117 accuracy: 0.370\n",
      "[1_4,    32/716] loss: 1.101 accuracy: 0.410\n",
      "[1_5,    40/716] loss: 1.096 accuracy: 0.428\n",
      "[1_6,    48/716] loss: 1.086 accuracy: 0.440\n",
      "[1_7,    56/716] loss: 1.081 accuracy: 0.451\n",
      "[1_8,    64/716] loss: 1.056 accuracy: 0.467\n",
      "[1_9,    72/716] loss: 1.064 accuracy: 0.477\n",
      "[1_10,    80/716] loss: 1.063 accuracy: 0.487\n",
      "[1_11,    88/716] loss: 1.085 accuracy: 0.494\n",
      "[1_12,    96/716] loss: 1.017 accuracy: 0.501\n",
      "[1_13,   104/716] loss: 1.018 accuracy: 0.512\n",
      "[1_14,   112/716] loss: 1.028 accuracy: 0.519\n",
      "[1_15,   120/716] loss: 1.011 accuracy: 0.520\n",
      "[1_16,   128/716] loss: 1.024 accuracy: 0.523\n",
      "[1_17,   136/716] loss: 0.919 accuracy: 0.528\n",
      "[1_18,   144/716] loss: 0.888 accuracy: 0.540\n",
      "[1_19,   152/716] loss: 0.946 accuracy: 0.547\n",
      "[1_20,   160/716] loss: 0.978 accuracy: 0.549\n",
      "[1_21,   168/716] loss: 0.814 accuracy: 0.556\n",
      "[1_22,   176/716] loss: 0.881 accuracy: 0.559\n",
      "[1_23,   184/716] loss: 0.953 accuracy: 0.560\n",
      "[1_24,   192/716] loss: 1.007 accuracy: 0.561\n",
      "[1_25,   200/716] loss: 0.986 accuracy: 0.562\n",
      "[1_26,   208/716] loss: 0.969 accuracy: 0.562\n",
      "[1_27,   216/716] loss: 0.972 accuracy: 0.561\n",
      "[1_28,   224/716] loss: 0.850 accuracy: 0.565\n",
      "[1_29,   232/716] loss: 0.794 accuracy: 0.572\n",
      "[1_30,   240/716] loss: 0.825 accuracy: 0.576\n",
      "[1_31,   248/716] loss: 0.884 accuracy: 0.578\n",
      "[1_32,   256/716] loss: 1.018 accuracy: 0.579\n",
      "[1_33,   264/716] loss: 0.804 accuracy: 0.582\n",
      "[1_34,   272/716] loss: 0.931 accuracy: 0.584\n",
      "[1_35,   280/716] loss: 0.939 accuracy: 0.587\n",
      "[1_36,   288/716] loss: 1.104 accuracy: 0.586\n",
      "[1_37,   296/716] loss: 0.924 accuracy: 0.587\n",
      "[1_38,   304/716] loss: 0.806 accuracy: 0.589\n",
      "[1_39,   312/716] loss: 0.892 accuracy: 0.591\n",
      "[1_40,   320/716] loss: 0.826 accuracy: 0.591\n",
      "[1_41,   328/716] loss: 0.845 accuracy: 0.594\n",
      "[1_42,   336/716] loss: 0.817 accuracy: 0.594\n",
      "[1_43,   344/716] loss: 0.907 accuracy: 0.593\n",
      "[1_44,   352/716] loss: 0.896 accuracy: 0.593\n",
      "[1_45,   360/716] loss: 0.794 accuracy: 0.595\n",
      "[1_46,   368/716] loss: 0.804 accuracy: 0.597\n",
      "[1_47,   376/716] loss: 0.782 accuracy: 0.599\n",
      "[1_48,   384/716] loss: 0.875 accuracy: 0.601\n",
      "[1_49,   392/716] loss: 0.725 accuracy: 0.603\n",
      "[1_50,   400/716] loss: 0.784 accuracy: 0.605\n",
      "[1_51,   408/716] loss: 0.821 accuracy: 0.605\n",
      "[1_52,   416/716] loss: 0.768 accuracy: 0.607\n",
      "[1_53,   424/716] loss: 0.876 accuracy: 0.606\n",
      "[1_54,   432/716] loss: 0.660 accuracy: 0.609\n",
      "[1_55,   440/716] loss: 0.818 accuracy: 0.609\n",
      "[1_56,   448/716] loss: 0.931 accuracy: 0.610\n",
      "[1_57,   456/716] loss: 0.907 accuracy: 0.610\n",
      "[1_58,   464/716] loss: 0.859 accuracy: 0.610\n",
      "[1_59,   472/716] loss: 0.729 accuracy: 0.612\n",
      "[1_60,   480/716] loss: 0.789 accuracy: 0.613\n",
      "[1_61,   488/716] loss: 0.767 accuracy: 0.613\n",
      "[1_62,   496/716] loss: 0.728 accuracy: 0.615\n",
      "[1_63,   504/716] loss: 0.603 accuracy: 0.617\n",
      "[1_64,   512/716] loss: 0.723 accuracy: 0.617\n",
      "[1_65,   520/716] loss: 0.704 accuracy: 0.619\n",
      "[1_66,   528/716] loss: 0.744 accuracy: 0.619\n",
      "[1_67,   536/716] loss: 0.672 accuracy: 0.620\n",
      "[1_68,   544/716] loss: 0.971 accuracy: 0.620\n",
      "[1_69,   552/716] loss: 0.853 accuracy: 0.619\n",
      "[1_70,   560/716] loss: 0.744 accuracy: 0.619\n",
      "[1_71,   568/716] loss: 0.758 accuracy: 0.620\n",
      "[1_72,   576/716] loss: 0.722 accuracy: 0.620\n",
      "[1_73,   584/716] loss: 0.707 accuracy: 0.621\n",
      "[1_74,   592/716] loss: 0.892 accuracy: 0.620\n",
      "[1_75,   600/716] loss: 0.859 accuracy: 0.620\n",
      "[1_76,   608/716] loss: 0.830 accuracy: 0.620\n",
      "[1_77,   616/716] loss: 0.596 accuracy: 0.622\n",
      "[1_78,   624/716] loss: 0.869 accuracy: 0.622\n",
      "[1_79,   632/716] loss: 0.832 accuracy: 0.622\n",
      "[1_80,   640/716] loss: 0.654 accuracy: 0.624\n",
      "[1_81,   648/716] loss: 0.717 accuracy: 0.625\n",
      "[1_82,   656/716] loss: 0.757 accuracy: 0.626\n",
      "[1_83,   664/716] loss: 0.735 accuracy: 0.627\n",
      "[1_84,   672/716] loss: 0.790 accuracy: 0.627\n",
      "[1_85,   680/716] loss: 1.000 accuracy: 0.626\n",
      "[1_86,   688/716] loss: 0.732 accuracy: 0.626\n",
      "[1_87,   696/716] loss: 0.653 accuracy: 0.627\n",
      "[1_88,   704/716] loss: 0.847 accuracy: 0.627\n",
      "[1_89,   712/716] loss: 0.778 accuracy: 0.627\n",
      "0.6279434850863422\n",
      "2020-07-31 12:33:19.270735  :  0.6441620310939952 is higher than the best(0). Saving the results at Results/distilberturk_AdamW/results.json\n",
      "Epoch:  1\n",
      "Loss: 77.62038746848702, Training accuracy:0.6279434850863422, Validation accuracy:0.6776212832550861, Test accuracy:0.7003768844221105\n",
      "Train accuracy: 0.6279434850863422\t Train Recall:0.5786593415455462\n",
      "Val accuracy: 0.6776212832550861\t Val Recall:0.6441620310939952\n",
      "Test accuracy: 0.7003768844221105\t Test Recall:0.6779191736906608\n",
      "[2_1,     8/716] loss: 0.556 accuracy: 0.750\n",
      "[2_2,    16/716] loss: 0.658 accuracy: 0.727\n",
      "[2_3,    24/716] loss: 0.753 accuracy: 0.719\n",
      "[2_4,    32/716] loss: 0.454 accuracy: 0.754\n",
      "[2_5,    40/716] loss: 0.632 accuracy: 0.741\n",
      "[2_6,    48/716] loss: 0.674 accuracy: 0.737\n",
      "[2_7,    56/716] loss: 0.589 accuracy: 0.750\n",
      "[2_8,    64/716] loss: 0.580 accuracy: 0.756\n",
      "[2_9,    72/716] loss: 0.568 accuracy: 0.759\n",
      "[2_10,    80/716] loss: 0.602 accuracy: 0.759\n",
      "[2_11,    88/716] loss: 0.613 accuracy: 0.760\n",
      "[2_12,    96/716] loss: 0.592 accuracy: 0.755\n",
      "[2_13,   104/716] loss: 0.483 accuracy: 0.756\n",
      "[2_14,   112/716] loss: 0.587 accuracy: 0.753\n",
      "[2_15,   120/716] loss: 0.531 accuracy: 0.749\n",
      "[2_16,   128/716] loss: 0.711 accuracy: 0.744\n",
      "[2_17,   136/716] loss: 0.511 accuracy: 0.748\n",
      "[2_18,   144/716] loss: 0.580 accuracy: 0.748\n",
      "[2_19,   152/716] loss: 0.612 accuracy: 0.747\n",
      "[2_20,   160/716] loss: 0.571 accuracy: 0.750\n",
      "[2_21,   168/716] loss: 0.595 accuracy: 0.751\n",
      "[2_22,   176/716] loss: 0.682 accuracy: 0.751\n",
      "[2_23,   184/716] loss: 0.417 accuracy: 0.754\n",
      "[2_24,   192/716] loss: 0.549 accuracy: 0.754\n",
      "[2_25,   200/716] loss: 0.500 accuracy: 0.756\n",
      "[2_26,   208/716] loss: 0.602 accuracy: 0.755\n",
      "[2_27,   216/716] loss: 0.645 accuracy: 0.758\n",
      "[2_28,   224/716] loss: 0.533 accuracy: 0.760\n",
      "[2_29,   232/716] loss: 0.584 accuracy: 0.759\n",
      "[2_30,   240/716] loss: 0.747 accuracy: 0.757\n",
      "[2_31,   248/716] loss: 0.498 accuracy: 0.759\n",
      "[2_32,   256/716] loss: 0.578 accuracy: 0.758\n",
      "[2_33,   264/716] loss: 0.518 accuracy: 0.758\n",
      "[2_34,   272/716] loss: 0.536 accuracy: 0.761\n",
      "[2_35,   280/716] loss: 0.564 accuracy: 0.760\n",
      "[2_36,   288/716] loss: 0.605 accuracy: 0.759\n",
      "[2_37,   296/716] loss: 0.679 accuracy: 0.757\n",
      "[2_38,   304/716] loss: 0.671 accuracy: 0.755\n",
      "[2_39,   312/716] loss: 0.482 accuracy: 0.756\n",
      "[2_40,   320/716] loss: 0.545 accuracy: 0.757\n",
      "[2_41,   328/716] loss: 0.570 accuracy: 0.759\n",
      "[2_42,   336/716] loss: 0.534 accuracy: 0.761\n",
      "[2_43,   344/716] loss: 0.594 accuracy: 0.762\n",
      "[2_44,   352/716] loss: 0.612 accuracy: 0.763\n",
      "[2_45,   360/716] loss: 0.493 accuracy: 0.765\n",
      "[2_46,   368/716] loss: 0.493 accuracy: 0.765\n",
      "[2_47,   376/716] loss: 0.529 accuracy: 0.765\n",
      "[2_48,   384/716] loss: 0.642 accuracy: 0.766\n",
      "[2_49,   392/716] loss: 0.629 accuracy: 0.766\n",
      "[2_50,   400/716] loss: 0.579 accuracy: 0.766\n",
      "[2_51,   408/716] loss: 0.539 accuracy: 0.766\n",
      "[2_52,   416/716] loss: 0.586 accuracy: 0.765\n",
      "[2_53,   424/716] loss: 0.632 accuracy: 0.764\n",
      "[2_54,   432/716] loss: 0.602 accuracy: 0.764\n",
      "[2_55,   440/716] loss: 0.508 accuracy: 0.765\n",
      "[2_56,   448/716] loss: 0.629 accuracy: 0.765\n",
      "[2_57,   456/716] loss: 0.704 accuracy: 0.763\n",
      "[2_58,   464/716] loss: 0.641 accuracy: 0.764\n",
      "[2_59,   472/716] loss: 0.592 accuracy: 0.765\n",
      "[2_60,   480/716] loss: 0.383 accuracy: 0.766\n",
      "[2_61,   488/716] loss: 0.443 accuracy: 0.767\n",
      "[2_62,   496/716] loss: 0.657 accuracy: 0.766\n",
      "[2_63,   504/716] loss: 0.596 accuracy: 0.768\n",
      "[2_64,   512/716] loss: 0.432 accuracy: 0.769\n",
      "[2_65,   520/716] loss: 0.425 accuracy: 0.769\n",
      "[2_66,   528/716] loss: 0.575 accuracy: 0.770\n",
      "[2_67,   536/716] loss: 0.422 accuracy: 0.771\n",
      "[2_68,   544/716] loss: 0.666 accuracy: 0.771\n",
      "[2_69,   552/716] loss: 0.418 accuracy: 0.772\n",
      "[2_70,   560/716] loss: 0.476 accuracy: 0.772\n",
      "[2_71,   568/716] loss: 0.614 accuracy: 0.771\n",
      "[2_72,   576/716] loss: 0.414 accuracy: 0.772\n",
      "[2_73,   584/716] loss: 0.477 accuracy: 0.772\n",
      "[2_74,   592/716] loss: 0.443 accuracy: 0.773\n",
      "[2_75,   600/716] loss: 0.402 accuracy: 0.775\n",
      "[2_76,   608/716] loss: 0.597 accuracy: 0.775\n",
      "[2_77,   616/716] loss: 0.410 accuracy: 0.776\n",
      "[2_78,   624/716] loss: 0.485 accuracy: 0.776\n",
      "[2_79,   632/716] loss: 0.502 accuracy: 0.776\n",
      "[2_80,   640/716] loss: 0.636 accuracy: 0.775\n",
      "[2_81,   648/716] loss: 0.411 accuracy: 0.776\n",
      "[2_82,   656/716] loss: 0.617 accuracy: 0.775\n",
      "[2_83,   664/716] loss: 0.589 accuracy: 0.775\n",
      "[2_84,   672/716] loss: 0.507 accuracy: 0.776\n",
      "[2_85,   680/716] loss: 0.533 accuracy: 0.777\n",
      "[2_86,   688/716] loss: 0.722 accuracy: 0.776\n",
      "[2_87,   696/716] loss: 0.545 accuracy: 0.776\n",
      "[2_88,   704/716] loss: 0.528 accuracy: 0.776\n",
      "[2_89,   712/716] loss: 0.442 accuracy: 0.777\n",
      "0.7762079190650619\n",
      "2020-07-31 12:35:17.674752  :  0.6453686740157611 is higher than the best(0.6441620310939952). Saving the results at Results/distilberturk_AdamW/results.json\n",
      "Epoch:  2\n",
      "Loss: 49.860295702237636, Training accuracy:0.7762079190650619, Validation accuracy:0.6823161189358372, Test accuracy:0.6959798994974874\n",
      "Train accuracy: 0.7762079190650619\t Train Recall:0.7429452256723018\n",
      "Val accuracy: 0.6823161189358372\t Val Recall:0.6453686740157611\n",
      "Test accuracy: 0.6959798994974874\t Test Recall:0.6664441445159096\n",
      "[3_1,     8/716] loss: 0.363 accuracy: 0.812\n",
      "[3_2,    16/716] loss: 0.375 accuracy: 0.797\n",
      "[3_3,    24/716] loss: 0.372 accuracy: 0.812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b937c006186b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "batch_size = 8\n",
    "best_val_recall = 0\n",
    "accumulation_steps = 8\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    indices = np.arange(len(X_train_feat))\n",
    "    np.random.shuffle(indices)\n",
    "    train_outputs = torch.LongTensor([]).to(device)\n",
    "    for idx in range(math.ceil(len(X_train_feat)/batch_size)):\n",
    "        inputs_0 = X_train_feat[indices[idx*batch_size:min(len(X_train_feat), (idx+1)*batch_size)]].to(device)\n",
    "        input_attention = X_train_attention[indices[idx*batch_size:min(len(X_train_attention), (idx+1)*batch_size)]].to(device)\n",
    "        labels = y_train[indices[idx*batch_size:min(len(y_train), (idx+1)*batch_size)]].to(device)\n",
    "        \n",
    "        \n",
    "        outputs = sentiment_net(inputs_0, input_attention)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total+= len(labels)\n",
    "        train_outputs = torch.cat((train_outputs, predicted), 0)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps \n",
    "        loss.backward()\n",
    "        \n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            print('[%d_%d, %5d/%d] loss: %.3f accuracy: %.3f' %\n",
    "                  (epoch + 1, (idx + 1)//accumulation_steps, idx + 1, len(X_train_feat)//batch_size, running_loss, correct/total))\n",
    "            total_loss += running_loss\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    train_acc = correct/total\n",
    "    print(train_acc)\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.tensor([], device=device)\n",
    "        for idx in range(math.ceil(len(X_test_feat)/batch_size)):\n",
    "            inputs_0 = X_test_feat[idx*batch_size:min(len(X_test_feat), (idx+1)*batch_size)]\n",
    "            input_attention = X_test_attention[idx*batch_size:min(len(X_test_attention), (idx+1)*batch_size)]\n",
    "            inputs_0 = inputs_0.to(device)\n",
    "            input_attention = input_attention.to(device)\n",
    "            outputs = torch.cat((outputs, sentiment_net(inputs_0, input_attention)), 0)\n",
    "        _, predicted_test = torch.max(outputs.data, 1)\n",
    "        y_test = y_test.to(device)\n",
    "        total = y_test.size(0)\n",
    "        correct = (predicted_test == y_test).sum().item()\n",
    "        test_acc = correct/total\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch.tensor([], device=device)\n",
    "        val_loss = 0\n",
    "        for idx in range(math.ceil(len(X_dev_feat)/batch_size)):\n",
    "            inputs_0 = X_dev_feat[idx*batch_size:min(len(X_dev_feat), (idx+1)*batch_size)]\n",
    "            input_attention = X_dev_attention[idx*batch_size:min(len(X_dev_attention), (idx+1)*batch_size)]\n",
    "            labels = y_dev[idx*batch_size:min(len(y_dev), (idx+1)*batch_size)].to(device)\n",
    "            inputs_0 = inputs_0.to(device)\n",
    "            input_attention = input_attention.to(device)\n",
    "            out_0 = sentiment_net(inputs_0, input_attention)\n",
    "            outputs = torch.cat((outputs, out_0), 0)\n",
    "            val_loss += criterion(out_0, labels)\n",
    "        _, predicted_dev = torch.max(outputs.data, 1)\n",
    "        y_dev = y_dev.to(device)\n",
    "        total = y_dev.size(0)\n",
    "        correct = (predicted_dev == y_dev).sum().item()\n",
    "        val_acc = correct/total\n",
    "        val_recall = recall_score(predicted_dev.cpu(), y_dev.cpu(), average=\"macro\")\n",
    "    \n",
    "    test_recall = recall_score(predicted_test.cpu(), y_test.cpu(), average=\"macro\")\n",
    "    if val_recall>best_val_recall:\n",
    "        now = datetime.now()\n",
    "        print(f'{now}  :  {val_recall} is higher than the best({best_val_recall}). Saving the results at {results_filepath}')\n",
    "        # En iyi modeli kaydetmek için buradaki yorum kısmını kaldırın.\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch+1,\n",
    "#                 'model_state_dict': sentiment_net.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': total_loss\n",
    "#                 }, f'Models/distilberturk_emoji_pair_class_rec_{test_recall}_acc_{accuracy_score(predicted_test.cpu(), y_test.cpu())}_{epoch+1}_sigmoid.pt')\n",
    "        best_val_recall = val_recall\n",
    "        results = {\"Epoch\":epoch+1, \"Training Loss\":total_loss, \"Training Accuracy\":train_acc, \"Validation Accuracy\":val_acc, \"Test Accuracy\":test_acc, \"Train Recall\":recall_score(train_outputs.cpu(), y_train[indices].cpu(), average=\"macro\"), \"Validation Recall\":val_recall, \"Test Recall\":test_recall}\n",
    "        with open(results_filepath, \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "    print('Epoch: ',epoch+1)\n",
    "    print(f'Loss: {total_loss}, Training accuracy:{train_acc}, Validation accuracy:{val_acc}, Test accuracy:{test_acc}')\n",
    "    print(f'Train accuracy: {accuracy_score(train_outputs.cpu(), y_train[indices].cpu())}\\t Train Recall:{recall_score(train_outputs.cpu(), y_train[indices].cpu(), average=\"macro\")}')\n",
    "    print(f'Val accuracy: {accuracy_score(predicted_dev.cpu(), y_dev.cpu())}\\t Val Recall:{val_recall}')\n",
    "    print(f'Test accuracy: {accuracy_score(predicted_test.cpu(), y_test.cpu())}\\t Test Recall:{test_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
