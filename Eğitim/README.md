# EÄŸitim

Bu bÃ¶lÃ¼mde hem Ã¶n eÄŸitim (pretraining) aÅŸamasÄ± iÃ§in hem de ince ayar (finetuning) aÅŸamasÄ± iÃ§in gerekli kodlar paylaÅŸÄ±ldÄ±.

### Kodlar
*[Ä°nce Ayar.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C4%B0nce%20Ayar.ipynb)*: FarklÄ± transformer modelleri, optimizerlar, eÄŸitim oranlarÄ± ve weight decayler iÃ§in verilen bir konfigÃ¼rasyon iÃ§in BOUN Twitter veri seti Ã¼zerinden model eÄŸitiliyor.

*[Ä°nce Ayar Analizleri.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C4%B0nce%20Ayar%20Analizleri.ipynb)*: Belirtilen farklÄ± konfigÃ¼rasyonlarÄ±n karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± ve detaylÄ± analizinin paylaÅŸÄ±ldÄ±ÄŸÄ± notebook. 

*[Ã–n EÄŸitim - SÄ±nÄ±flandÄ±rma.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C3%96n%20E%C4%9Fitim%20-%20S%C4%B1n%C4%B1fland%C4%B1rma.ipynb)*: Emojileri kullanarak yapÄ±lan emoji kategorilerini tahmin eden ilk Ã¶n eÄŸitim tekniÄŸi. DetaylÄ± aÃ§Ä±klamasÄ± iÃ§in notebook'a veya aÅŸaÄŸÄ±ya bakabilirsiniz.

*[Ã–n EÄŸitim - EÅŸli SÄ±nÄ±flandÄ±rma.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C3%96n%20E%C4%9Fitim%20-%20E%C5%9Fli%20S%C4%B1n%C4%B1fland%C4%B1rma.ipynb)*: Emojileri kullanarak yapÄ±lan ikili sÄ±nÄ±flandÄ±rma yapan ikinci Ã¶n eÄŸitim tekniÄŸi. DetaylÄ± aÃ§Ä±klamasÄ± iÃ§in notebook'a veya aÅŸaÄŸÄ±ya bakabilirsiniz.


## Ä°nce Ayar

Bu Ã§alÄ±ÅŸmada Ã§eÅŸitli transformer modellerini farklÄ± ince ayar (finetuning) eÄŸitim senaryolarÄ± iÃ§in **doÄŸrulama setindeki doÄŸruluk skoru** Ã¼zerinden karÅŸÄ±laÅŸtÄ±rdÄ±k. Toplamda 216 farklÄ± senaryo karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

### 1. Transformers 

KullanÄ±lan transformer modelleri:
* [Multilingual BERT](https://github.com/google-research/bert) : Multilingual BERT, Google tarafÄ±ndan 104 dil iÃ§in Wikipedia Ã¼zerinden eÄŸitilmiÅŸ bir modeldir.
* [BerTurk](https://huggingface.co/dbmdz/bert-base-turkish-cased) : Stefan Schweter tarafÄ±ndan 35 GB'lÄ±k Ã§eÅŸitli TÃ¼rkÃ§e kaynaklardan oluÅŸturularak eÄŸitilmiÅŸ, BERT'in base modeli ile aynÄ± bÃ¼yÃ¼klÃ¼ÄŸe sahip bir modeldir.
* [Distilled (DamÄ±tÄ±lmÄ±ÅŸ) BerTurk](https://huggingface.co/dbmdz/distilbert-base-turkish-cased) : YukarÄ±daki transformer modelinin, distillation [1] tekniÄŸiyle baÅŸarÄ±mÄ± ciddi seviyede korunarak, damÄ±tÄ±lmÄ±ÅŸ bir halidir.


Modellerin karÅŸÄ±laÅŸtÄ±rmasÄ±:

|                   | Katman SayÄ±sÄ± | Kafa SayÄ±sÄ± | SaklÄ± Katman BÃ¼yÃ¼klÃ¼ÄŸÃ¼ | Toplam Parametre SayÄ±sÄ± |
|-------------------|:-------------:|:-----------:|:----------------------:|:-----------------------:|
| Multilingual BERT |       12      |      12     |           768          |           110M          |
| BerTurk           |       12      |      12     |           768          |           110M          |
| Distilled BerTurk |       6       |      12     |           768          |           66M           |

### 2. Ä°yileÅŸtirici (Optimizer)

Ã‡eÅŸitli optimizerlar bu Ã§alÄ±ÅŸmada denenmiÅŸtir:
* [AdamW](https://arxiv.org/pdf/1711.05101.pdf)
* [Adam](https://arxiv.org/pdf/1412.6980.pdf)
* [SGD](https://projecteuclid.org/euclid.aoms/1177729392)


### 3. Ã–ÄŸrenme OranÄ± & Kilo KaybÄ± (?) (Learning Rate & Weight Decay)

1e-3, 1e-4, 1e-5, 1e-6 Ã¶ÄŸrenme oranlarÄ± ve 0, 0.01, 0.1 weight decay deÄŸerleri ince ayar senaryolarÄ±nda araÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

### 4. Ã–n Ä°ÅŸleme

Her ne kadar derin Ã¶ÄŸrenme tekniklerinin popÃ¼lerleÅŸmesiyle Ã¶n iÅŸleme (preprocessing) tekniklerinin Ã¶nemi azalsa da bu Ã§alÄ±ÅŸmada basit bir ÅŸekilde URL silmenin etkisi de incelenmiÅŸtir.

En iyi sonuÃ§ alan modelin detayÄ±:

* Transformer: **Distil BerTurk**
* Optimizer: **AdamW**
* Learning Rate: **0.00001**
* Weight Decay: **0.01**
* Ã–n iÅŸlem: **Url Silmek**

![](https://live.staticflickr.com/65535/50173524568_44e146ba0c_b.jpg)

DetaylÄ± sonuÃ§lar iÃ§in: [Ä°nce Ayar Analizleri.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C4%B0nce%20Ayar%20Analizleri.ipynb)


## Ã–n EÄŸitim

Bu Ã§alÄ±ÅŸmada Ã¶n eÄŸitim (pretraining) aÅŸamasÄ± olarak iki farklÄ± yaklaÅŸÄ±m izlenmiÅŸtir. Bu yaklaÅŸÄ±mlarda Ã§ok sayÄ±da emojili TÃ¼rkÃ§e twit verisi kullanÄ±lmÄ±ÅŸtÄ±r.

[**Mandalina TÃ¼rkÃ§e Emoji Veri Seti**](https://drive.google.com/file/d/1z9un8GnaHH2--07yS0O_4jM88vNG14nv/view?usp=sharing)

* 767.197 twit
* 67 emoji
* 8 emoji kategorisi

### 1. [Ã–n EÄŸitim - SÄ±nÄ±flandÄ±rma.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C3%96n%20E%C4%9Fitim%20-%20S%C4%B1n%C4%B1fland%C4%B1rma.ipynb)
Verilen bir emojili twit iÃ§in emojilerin Ã§Ä±karÄ±lmÄ±ÅŸ hali Ã¼zerinden emoji kategorisinin tahmin edilmesi hedeflenmiÅŸtir.

* MaskelenmiÅŸ Dil Modeli
* Emoji Kategori Tahmini

### 2. [Ã–n EÄŸitim - EÅŸli SÄ±nÄ±flandÄ±rma.ipynb](https://github.com/akoksal/Turkce-Duygu-Takibi/blob/master/Eg%CC%86itim/%C3%96n%20E%C4%9Fitim%20-%20E%C5%9Fli%20S%C4%B1n%C4%B1fland%C4%B1rma.ipynb)
Bu Ã§alÄ±ÅŸmada ilk Ã¶n eÄŸitim notebook'unda olduÄŸu gibi maskeli dil modeli eÄŸitilmekte ve ek olarak emoji sÄ±nÄ±flandÄ±rmayÄ± ise eÅŸli bir ÅŸekilde yapÄ±yoruz.

Bu yaklaÅŸÄ±m yine BERT'te olan bir sonraki cÃ¼mleyi sÄ±nÄ±flandÄ±rmaya (next sentence prediction) benziyor. Burada ikili bir ÅŸekilde eÅŸleÅŸtirdiÄŸimiz emojileri siliniÅŸ twitlerden, bu iki twitin aynÄ± emojiyi iÃ§erip iÃ§ermediÄŸini tahmin eden bir eÅŸli sÄ±nÄ±flandÄ±rma gÃ¶revi oluÅŸturmaktayÄ±z.

Ã–rneÄŸin,

Twit 1: Bu tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ± ğŸ˜¦

Twit 2: Ã‡ok mutluyum ğŸ™ƒ

Twit 3: SÄ±nav iÃ§in Ã§ok stresliyim ğŸ˜¦

iÃ§in

*("Bu tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ±", "Ã‡ok mutluyum")* ikilisi farklÄ± kategorilerden emojiler iÃ§erdiÄŸi iÃ§in bunlarÄ±n farklÄ± olduÄŸunu modelimizin bu ÅŸekilde (0) ayÄ±rt etmesini istiyoruz.

*("Bu tam bir hayal kÄ±rÄ±klÄ±ÄŸÄ±", "SÄ±nav iÃ§in Ã§ok stresliyim")* ikilisi aynÄ± kategoriden emojiler iÃ§erdiÄŸi iÃ§in bunlarÄ±n aynÄ± olduÄŸunu modelimizin bu ÅŸekilde (1) ayÄ±rt etmesini istiyoruz.


![](https://live.staticflickr.com/65535/50174348517_25545f3bb8.jpg)

*** 
En iyi Ã§Ä±kan Distilled BerTurk modeli iÃ§in emojili Ã¶n eÄŸitimlerde istatistiksel olarak anlamlÄ± bir geliÅŸme gÃ¶zlenmedi. Bunun, BOUN Twitter Verisinin potansiyel olarak iÃ§erdiÄŸi gÃ¼rÃ¼ltÃ¼lÃ¼ (noisy) verilerden kaynaklandÄ±ÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yoruz. Sorgu temelli yapÄ±lan Ã§alÄ±ÅŸmalarda daha iyi sonuÃ§lar gÃ¶zlemlendi.
